{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c50f611-5a3f-4183-b61c-b318e6888d62",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src=\"https://blog.ucsp.edu.pe/hs-fs/hubfs/logo-de-UCSP-16.png?width=250&height=133&name=logo-de-UCSP-16.png\" width=\"300\" >\n",
    "<br>\n",
    "<a href=\"#\">\n",
    "  <img src=\"https://img.shields.io/badge/Articial-Intelligence-orange\" alt=\"Support Ukraine - Help Provide Humanitarian Aid to Ukraine.\" />\n",
    "  <img src=\"https://img.shields.io/badge/CCOMP-UCSP-brightgreen\" alt=\"Support Ukraine - Help Provide Humanitarian Aid to Ukraine.\" />\n",
    "</a>\n",
    "</div>\n",
    "A Transformer network, DETR model, is used to analyse the objects in real time, and a panoptic model is used for classification or context understanding.\n",
    "\n",
    "## Integrantes\n",
    "* [Chillitupa Quispihuanca, Alfred Addison](projects/)\n",
    "* [MuÃ±oz Curi, Rayver Aimar](projects/)\n",
    "* [Gomez del Carpio, Alexander](projects/)\n",
    "* [Quispe Salcedo, Josep](projects/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fc1e81-6d4b-4ca6-a80a-c54a363dbccd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Research DETR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3445c8-8bcf-4b11-a6f9-eda8d81a1c2d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27572a72-fe33-444e-a471-ccc26a1ca6e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81438e11-d79f-4909-89c3-93b60c2051e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8724317b-f792-4874-8fed-a52684f270f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/ultralytics/yolov5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc4f6dd-d8c3-4641-bdff-323013a951df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install -r yolov5/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d3194b-742a-42eb-b538-88f03d3e1f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install git+https://github.com/cocodataset/panopticapi.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24505878-fc5a-49f3-9808-c9c385e2adf5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Python Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280ed5ce-ced4-47a5-8311-a87af34d6199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import requests\n",
    "import io\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b2fec5-ba59-439d-af83-7a3cf7ec6944",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Computer Vision Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3fc612-9552-4dbe-a592-3809c116dcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from torch import nn\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ec3365-cca3-4fef-ad5d-97ebd72eeb1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Transformers Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24f954a-cea2-40aa-9307-8f33343f4aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "torch.set_grad_enabled(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f0fb6b-dbe6-42e7-b623-08a3f6445eb9",
   "metadata": {},
   "source": [
    "### Transformers Panoptical Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c80364-2744-4bbf-8456-f152dca5e7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import panopticapi\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "\n",
    "from panopticapi.utils import id2rgb, rgb2id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d59fb4-28b0-4a42-a0ec-5778e6c51755",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model DETR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c291a6-47d1-4ccc-8aa6-095ccd109c37",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setup Coco Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d04b332-495d-4b5d-996c-1e7235045375",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = [\n",
    "    'N/A', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A',\n",
    "    'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n",
    "    'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack',\n",
    "    'umbrella', 'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n",
    "    'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
    "    'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass',\n",
    "    'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n",
    "    'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',\n",
    "    'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A',\n",
    "    'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n",
    "    'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A',\n",
    "    'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n",
    "    'toothbrush'\n",
    "]\n",
    "\n",
    "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
    "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704c9f79-103f-4573-8b4f-3aaa4c8ef347",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Input Image Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751351e5-72fb-4aa7-98c3-57b01257f423",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "    T.Resize(800),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99ea42a-15b5-4254-8dcb-d3de86510aca",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Bounding box and post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e15855-5892-4c32-930e-2f6a16995bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=1)\n",
    "\n",
    "def rescale_bboxes(out_bbox, size):\n",
    "    img_w, img_h = size\n",
    "    b = box_cxcywh_to_xyxy(out_bbox)\n",
    "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
    "    return b\n",
    "\n",
    "def plot_results(pil_img, prob, boxes):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    plt.imshow(pil_img)\n",
    "    ax = plt.gca()\n",
    "    colors = COLORS * 100\n",
    "    for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), colors):\n",
    "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                   fill=False, color=c, linewidth=3))\n",
    "        cl = p.argmax()\n",
    "        text = f'{CLASSES[cl]}: {p[cl]:0.2f}'\n",
    "        ax.text(xmin, ymin, text, fontsize=15,\n",
    "                bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "    plt.axis('off')\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0b0e08-a474-4d83-9d52-da2edb6478ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f99fd3-c531-40c9-9a63-c25350fe7f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load('facebookresearch/detr', 'detr_resnet50', pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292325b8-f8a4-4570-900b-4d703f0e8e22",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Transformer Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449830ce-fb34-4a70-95f4-1f34010d1825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_detect( _frame, _accuracy = 0.5 ):\n",
    "    _imageRGB = cv2.cvtColor(np.flip(_frame, 1), cv2.COLOR_BGR2RGB)\n",
    "    _im = Image.fromarray(_imageRGB)\n",
    "    _img = transform(_im).unsqueeze(0)\n",
    "\n",
    "    _results = model(_img)\n",
    "    _probas = _results['pred_logits'].softmax(-1)[0, :, :-1]\n",
    "    _keep = _probas.max(-1).values > _accuracy\n",
    "    \n",
    "    _bboxes_scaled = rescale_bboxes(_results['pred_boxes'][0, _keep], _im.size)\n",
    "    plot_results(_im, _probas[_keep], _bboxes_scaled)\n",
    "    \n",
    "    return [_im,_probas,_keep,_bboxes_scaled, _results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a381690-262e-4418-8625-81b8ea147e54",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Map of Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c70592-c736-43c1-8137-ccc191a0bb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_mapsss( data ):\n",
    "    _img = transform(data[0]).unsqueeze(0)\n",
    "    conv_features, enc_attn_weights, dec_attn_weights = [], [], []\n",
    "\n",
    "    hooks = [ model.backbone[-2].register_forward_hook( lambda self, input, output: conv_features.append(output) ), model.transformer.encoder.layers[-1].self_attn.register_forward_hook(\n",
    "            lambda self, input, output: enc_attn_weights.append(output[1])), model.transformer.decoder.layers[-1].multihead_attn.register_forward_hook(\n",
    "            lambda self, input, output: dec_attn_weights.append(output[1])),\n",
    "            ]\n",
    "\n",
    "    # propagate through the model\n",
    "    outputs = model(_img)\n",
    "    \n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "        \n",
    "    conv_features = conv_features[0]\n",
    "    enc_attn_weights = enc_attn_weights[0]\n",
    "    dec_attn_weights = dec_attn_weights[0]\n",
    "    \n",
    "    \n",
    "    h, w = conv_features['0'].tensors.shape[-2:]\n",
    "\n",
    "    fig, axs = plt.subplots(ncols=len(data[3]), nrows=2, figsize=(22, 7))\n",
    "    colors = COLORS * 100\n",
    "    for idx, ax_i, (xmin, ymin, xmax, ymax) in zip(data[2].nonzero(), axs.T, data[3]):\n",
    "        ax = ax_i[0]\n",
    "        ax.imshow(dec_attn_weights[0, idx].view(h, w))\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f'query id: {idx.item()}')\n",
    "        ax = ax_i[1]\n",
    "        ax.imshow(data[0])\n",
    "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, fill=False, color='blue', linewidth=3))\n",
    "        ax.axis('off')\n",
    "        ax.set_title(CLASSES[data[1][idx].argmax()])\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    return [conv_features, enc_attn_weights, dec_attn_weights]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67aaab3e-c97f-4eb7-b5e1-e9e4f5665cb4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Sampling Self Atention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e576e212-0084-4cf0-9ad3-c39be8e0ec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention(data, conv_data):\n",
    "    _img = transform(data[0]).unsqueeze(0)\n",
    "    f_map = conv_data[0]['0']\n",
    "    shape = f_map.tensors.shape[-2:]\n",
    "\n",
    "    sattn = conv_data[1][0].reshape(shape + shape)\n",
    "    fact = 32\n",
    "\n",
    "    idxs = [(200, 200), (450, 520), (540, 900), (540, 800)]\n",
    "\n",
    "    fig = plt.figure(constrained_layout=True, figsize=(25 * 0.7, 8.5 * 0.7))\n",
    "\n",
    "    gs = fig.add_gridspec(2, 4)\n",
    "    axs = [\n",
    "        fig.add_subplot(gs[0, 0]),\n",
    "        fig.add_subplot(gs[1, 0]),\n",
    "        fig.add_subplot(gs[0, -1]),\n",
    "        fig.add_subplot(gs[1, -1]),\n",
    "    ]\n",
    "\n",
    "    for idx_o, ax in zip(idxs, axs):\n",
    "        idx = (idx_o[0] // fact, idx_o[1] // fact)\n",
    "        ax.imshow(sattn[..., idx[0], idx[1]], cmap='cividis', interpolation='nearest')\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f'self-attention{idx_o}')\n",
    "\n",
    "    fcenter_ax = fig.add_subplot(gs[:, 1:-1])\n",
    "    fcenter_ax.imshow(data[0])\n",
    "    for (y, x) in idxs:\n",
    "        scale = data[0].height / _img.shape[-2]\n",
    "        x = ((x // fact) + 0.5) * fact\n",
    "        y = ((y // fact) + 0.5) * fact\n",
    "        fcenter_ax.add_patch(plt.Circle((x * scale, y * scale), fact // 2, color='r'))\n",
    "        fcenter_ax.axis('off')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f67a6e1-3ba9-4d8d-8950-3ca736dad4e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7b2bb9-5cd9-4a68-beec-4f87eaecc977",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#url = 'https://www.teknofilo.com/wp-content/uploads/2019/12/41512060690_57ed31344c_h1-1280x754.jpg'\n",
    "#im = Image.open(requests.get(url, stream=True).raw)\n",
    "#im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b37ffad-3193-4cb9-bc4a-68043a4bfdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#img = transform(im).unsqueeze(0)\n",
    "\n",
    "#outputs = model(img)\n",
    "\n",
    "#probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]\n",
    "#keep = probas.max(-1).values > 0.5\n",
    "\n",
    "#bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], im.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137eb790-605a-470c-9144-5d09cdfbc24c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#plot_results(im, probas[keep], bboxes_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1766641a-9a94-4881-ac9d-9a4c67e0dc02",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model DETR Object Tracking and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9145bdaa-2419-4653-8cfa-458ec6450b6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "i = 1\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if i == 150:\n",
    "        _im = transformer_detect(frame)\n",
    "        _conv_data = transformer_mapsss( _im )\n",
    "        self_attention( _im, _conv_data )     \n",
    "    \n",
    "    cv2.imshow('YOLO', np.flip(frame, 1))\n",
    "   \n",
    "    i = (i + 1)%250;\n",
    "    \n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8cbade-62a9-4524-a300-0b53b874c6ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Panoptical DETR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6ca0fc-7ccc-446f-90f1-6709dd565b92",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d186d914-53d7-4162-a726-fb122681d2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, postprocessor = torch.hub.load('facebookresearch/detr', 'detr_resnet101_panoptic', pretrained=True, return_postprocessor=True, num_classes=250)\n",
    "#model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73235fa5-10f4-45b3-b7fe-48c2b0683912",
   "metadata": {},
   "source": [
    "### Computing Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c89f0ef-0b3f-413b-852d-baf8d0abd975",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "    T.Resize(800),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960227fe-e85f-4e17-95b1-7ccbd15b27e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computing_Mask( _frame ):\n",
    "    _imageRGB = cv2.cvtColor(np.flip(_frame, 1), cv2.COLOR_BGR2RGB)\n",
    "    _im = Image.fromarray(_imageRGB)\n",
    "    _img = transform(_im).unsqueeze(0)\n",
    "    out = model(_img)\n",
    "       \n",
    "    scores = out[\"pred_logits\"].softmax(-1)[..., :-1].max(-1)[0]\n",
    "    \n",
    "    keep = scores > 0.85\n",
    "\n",
    "    ncols = 5\n",
    "   \n",
    "    fig, axs = plt.subplots(ncols=ncols, nrows=math.ceil(keep.sum().item() / ncols), squeeze=False)\n",
    "\n",
    "    for line in axs:\n",
    "        for a in line:\n",
    "            a.axis('off')\n",
    "    \n",
    "    for i, mask in enumerate(out[\"pred_masks\"][keep]):\n",
    "        ax = axs[i // ncols, i % ncols]\n",
    "        ax.imshow(mask, cmap=\"cividis\")\n",
    "        ax.axis('off')\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    \n",
    "    result = postprocessor(out, torch.as_tensor(_img.shape[-2:]).unsqueeze(0))[0]\n",
    "    palette = itertools.cycle(sns.color_palette())\n",
    "\n",
    "    panoptic_seg = Image.open(io.BytesIO(result['png_string']))\n",
    "    panoptic_seg = np.array(panoptic_seg, dtype=np.uint8).copy()\n",
    "\n",
    "    panoptic_seg_id = rgb2id(panoptic_seg)\n",
    "\n",
    "    panoptic_seg[:, :, :] = 0\n",
    "    for id in range(panoptic_seg_id.max() + 1):\n",
    "      panoptic_seg[panoptic_seg_id == id] = np.asarray(next(palette)) * 255\n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.imshow(panoptic_seg)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6747763-438a-44b8-8d12-d55fa47c8bd6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model DETR Panoptical Object Tracking and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7a014b-2540-4156-891b-c88ebbe6163f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "i = 1\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if i == 150:\n",
    "        _out = computing_Mask(frame)\n",
    "        #color_Panoptical(_out, frame)\n",
    "        \n",
    "    \n",
    "    cv2.imshow('YOLO', np.flip(frame, 1))\n",
    "   \n",
    "    i = (i + 1)%250;\n",
    "    \n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
